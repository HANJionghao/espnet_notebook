{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR + LLM Fine-tuning with ESPnetEZ\n",
    "\n",
    "This Jupyter notebook provides a step-by-step guide on using the ESPnetEZ trainer to fine-tune ASR + LLM. In this demonstration, we will leverage the `MuST-C-v2` dataset (English to German subset) to fine-tune this cascade Speech Translation (ST) system.\n",
    "\n",
    "In this notebook, we assume that you have already downloaded the `MuST-C-v2` dataset and created the dump file using the recipe. If you haven't done this and are unfamiliar with the recipes provided in ESPnet, you can refer to the data preparation sections in the `train_from_scratch.ipynb` or `finetune_owsm.ipynb` notebooks in the ASR demos.\n",
    "\n",
    "Author: Masao Someki [@Masao-Someki](https://github.com/Masao-Someki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Let's install espnet and transformers if you haven't installed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U espnet\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import necessary libraries and set several hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from espnet2.layers.create_adapter_fn import create_lora_adapter\n",
    "from espnet2.asr.espnet_model import ESPnetASRModel\n",
    "from espnet2.train.dataset import kaldi_loader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import espnetez as ez\n",
    "\n",
    "CONFIG = \"owsm_finetune_base\"\n",
    "TRAIN_KEY = \"huggingface_cascade\"\n",
    "FINETUNE_MODEL = \"pyf98/librispeech_100_e_branchformer\"\n",
    "HF_MODEl = \"google-t5/t5-base\"\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "DUMP_DIR = \"./dump/raw\"\n",
    "EXP_DIR = f\"./exp/train_{TRAIN_KEY}\"\n",
    "STATS_DIR = f\"./exp/stats_{TRAIN_KEY}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this demo, let's define our custom dataset using the prepared dump files. This dataset will load the audio and text data from the dump files and convert the text data for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, data_path, is_train=True):\n",
    "        self.data_path = data_path\n",
    "        if is_train:\n",
    "            data_path = f\"{data_path}/train.en-de_sp\"\n",
    "        else:\n",
    "            data_path = f\"{data_path}/dev.en-de\"\n",
    "        \n",
    "        self.data = {}\n",
    "        with open(f\"{data_path}/text.tc.de\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, translated = line.strip().split(maxsplit=1)\n",
    "                translated = translated.replace(\" &apos;\", \"'\")\\\n",
    "                                       .replace(\" &quot;\", '\"')\\\n",
    "                                       .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id] = {\n",
    "                    'translated': translated\n",
    "                }\n",
    "        \n",
    "        with open(f\"{data_path}/text\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, text = line.strip().split(maxsplit=1)\n",
    "                text = text.replace(\" &apos;\", \"'\")\\\n",
    "                           .replace(\" &quot;\", '\"')\\\n",
    "                           .replace(\" &amp;\", \"&\")\n",
    "                self.data[audio_id]['text'] = text\n",
    "        \n",
    "        self.keys = list(self.data.keys())\n",
    "        self.loader = kaldi_loader(f\"{data_path}/wav.scp\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # This output will be fed into the lambda function in `data_info`.\n",
    "        idx = int(idx)\n",
    "        return {\n",
    "            'speech': self.loader[self.keys[idx]].astype(np.float32),\n",
    "            'text': self.data[self.keys[idx]]['text'],\n",
    "            'translated': self.data[self.keys[idx]]['translated']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "\n",
    "Next, let's prepare the `build_model_fn` function for the Trainer. We will define our custom model for ASR + LLM training. In this demo, we will train the ASR model with the ASR text, and the LLM will be fine-tuned with the translated text and ASR output.\n",
    "\n",
    "We will use the `ESPnetASRModel` for training. Currently, we don't have a specific class to support custom models, so we will leverage the existing ASR class.\n",
    "\n",
    "The `forward` method will take the output of the `data_info` and `_lengths` tensors, and output the loss for training. I have also added logging functionality to track the training progress inside the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomFinetuneModel(ESPnetASRModel):\n",
    "    def __init__(self, model, log_every=500):\n",
    "        super().__init__(\n",
    "            vocab_size=model.vocab_size,\n",
    "            token_list=model.token_list,\n",
    "            frontend=model.frontend,\n",
    "            specaug=model.specaug,\n",
    "            normalize=model.normalize,\n",
    "            preencoder=model.preencoder,\n",
    "            encoder=model.encoder,\n",
    "            postencoder=model.postencoder,\n",
    "            decoder=model.decoder,\n",
    "            ctc=model.ctc,\n",
    "            ctc_weight=model.ctc_weight,\n",
    "            interctc_weight=model.interctc_weight,\n",
    "            ignore_id=model.ignore_id,\n",
    "            joint_network=None,\n",
    "            lsm_weight=0.0,\n",
    "            length_normalized_loss=False,\n",
    "            report_cer=False,\n",
    "            report_wer=False,\n",
    "            sym_space=\"<space>\",\n",
    "            sym_blank=\"<blank>\",\n",
    "            sym_sos = \"<sos>\",\n",
    "            sym_eos = \"<eos>\",\n",
    "            # sym_sop = \"<sop>\",  # start of prev\n",
    "            # sym_na = \"<na>\",  # not available\n",
    "            extract_feats_in_collect_stats=model.extract_feats_in_collect_stats,\n",
    "        )\n",
    "        self.iter_count = 0\n",
    "        self.log_every = log_every\n",
    "        self.log_stats = {\n",
    "            'loss': 0.0,\n",
    "            'acc': 0.0,\n",
    "            'lm_loss': 0.0,\n",
    "        }\n",
    "        self.lm = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            HF_MODEl,\n",
    "            device_map = 'cuda'\n",
    "        )\n",
    "        self.lm_tokenizer = AutoTokenizer.from_pretrained(HF_MODEl)\n",
    "    \n",
    "    def get_text(self, token_ids):\n",
    "        return [\n",
    "            tokenizer.tokens2text(converter.ids2tokens(token_id))\n",
    "            for token_id in token_ids\n",
    "        ]\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        speech: torch.Tensor,\n",
    "        speech_lengths: torch.Tensor,\n",
    "        text: torch.Tensor,\n",
    "        text_lengths: torch.Tensor,\n",
    "        text_spk2: torch.Tensor,\n",
    "        text_spk2_lengths: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # compute ASR loss\n",
    "        loss, stats, weight = super().forward(\n",
    "            speech, speech_lengths,\n",
    "            text, text_lengths,\n",
    "        )\n",
    "\n",
    "        # compute hf loss\n",
    "        target_texts = self.get_text(text_spk2)\n",
    "        target_tokens = self.lm_tokenizer(target_texts, return_tensors=\"pt\", padding=True).input_ids.to(speech.device)\n",
    "        input_texts = self.get_text(text)\n",
    "        input_tokens = self.lm_tokenizer(input_texts, return_tensors=\"pt\", padding=True).input_ids.to(speech.device)\n",
    "        lm_output = self.lm(input_ids=input_tokens, labels=target_tokens)\n",
    "\n",
    "        # Add lm loss to ASR loss\n",
    "        loss += lm_output.loss\n",
    "        stats['lm_loss'] = lm_output.loss.detach()\n",
    "\n",
    "        self.log_stats['loss'] += stats['loss'].item()\n",
    "        self.log_stats['acc'] += stats['acc'].item()\n",
    "        self.log_stats['lm_loss'] += lm_output.loss.item()\n",
    "\n",
    "        self.iter_count += 1\n",
    "        if self.iter_count % self.log_every == 0:\n",
    "            _loss = self.log_stats['loss'] / self.log_every\n",
    "            _acc = self.log_stats['acc'] / self.log_every\n",
    "            lm_loss = self.log_stats['lm_loss'] / self.log_every\n",
    "            print(f\"[{self.iter_count}] - loss: {_loss:.3f} - acc: {_acc:.3f} - lm_loss: {lm_loss:.3f}\")\n",
    "            self.log_stats['loss'] = 0.0\n",
    "            self.log_stats['acc'] = 0.0\n",
    "            self.log_stats['lm_loss'] = 0.0\n",
    "\n",
    "        return loss, stats, weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define the `data_info` and `build_model_fn` functions for the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = {\n",
    "    \"speech\": lambda d : d['speech'],\n",
    "    \"text\": lambda d : tokenize(d['text'].upper()),\n",
    "    \"text_spk2\": lambda d : tokenize(d['translated'].upper()),\n",
    "} \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def build_model_fn(args):\n",
    "    pretrained_model = Speech2Text.from_pretrained(\n",
    "        FINETUNE_MODEL,\n",
    "        beam_size=10,\n",
    "    )\n",
    "    model = pretrained_model.asr_model\n",
    "    model.train()\n",
    "    model = CustomFinetuneModel(model, log_every=20)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, let's define the training configuration, instanciate the dataset and trainer and start the training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    "    beam_size=10,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "\n",
    "training_config = vars(pretrained_model.asr_train_args)\n",
    "del pretrained_model\n",
    "\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "    \"asr\",\n",
    "    training_config,\n",
    "    f\"conf/{CONFIG}.yaml\"\n",
    ")\n",
    "finetune_config['multiple_iterator'] = False\n",
    "\n",
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=True)\n",
    "dev_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=False)\n",
    "\n",
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "dev_dataset = ez.dataset.ESPnetEZDataset(dev_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task=\"asr\",\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=dev_dataset,\n",
    "    data_info=data_info,\n",
    "    build_model_fn=build_model_fn,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
