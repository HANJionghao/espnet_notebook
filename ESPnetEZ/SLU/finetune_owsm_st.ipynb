{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OWSM finetuning for Spoken Language Understanding task\n",
    "This Jupyter notebook provides a step-by-step guide on using the ESPnetEZ module to finetune owsm model. In this demonstration, we will leverage the `SLURP` dataset (intent classification task) to finetune an OWSM model for SLU task. \n",
    "\n",
    "This demo will focus on how to add new tokens to the pre-traied OWSM model for the new task, intent classification. For the other sections such as data preparation, training, and evaluation, please refer to the other notebooks.\n",
    "\n",
    "In this notebook, we assume that you have already downloaded the `SLUPR` dataset, and created the dump file using the recipe. If you haven't, and you have never used recipe prepared in espnet, you can refer to the data preparation section of `train_from_scratch.ipynb` or `finetune_owsm.ipynb` notebooks in ASR demos.\n",
    "\n",
    "Author: Masao Someki [@Masao-Someki](https://github.com/Masao-Someki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install espnet if you haven't it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U espnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import necessary libraries and set several hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from espnet2.layers.create_adapter_fn import create_lora_adapter\n",
    "import argparse\n",
    "\n",
    "import espnetez as ez\n",
    "\n",
    "\n",
    "FINETUNE_MODEL = \"espnet/owsm_v3.1_ebf_base\"\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "DUMP_DIR = \"./dump/raw\"\n",
    "STATS_DIR = \"./exp/stats_owsm\"\n",
    "\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\n",
    "    \"<intent>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new token to the pre-trained OWSM tokenizer\n",
    "\n",
    "Here we will add new `<intent>` token to the pre-trained OWSM tokenizer. We prepared the `add_special_tokens` function from `espnet2.preprocess` module to add new tokens to the pretrained tokenizer, converter, and the Embedding layer.\n",
    "\n",
    "We will use the Embedding layer for the `build_model_fn` function to replace the pre-trained Embedding layer with the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.s2t_inference import Speech2Text\n",
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    "    # category_sym=\"<en>\",\n",
    "    beam_size=10,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "\n",
    "# Add new <intent_cls> token after ST-related tokens\n",
    "tokenizer, converter, _ = ez.preprocess.add_special_tokens(\n",
    "    tokenizer, converter, pretrained_model.s2t_model.decoder.embed[0],\n",
    "    ADDITIONAL_SPECIAL_TOKENS, insert_after=\"<st_zho>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And load configuration of pre-trained model. before deleting it.\n",
    "training_config = vars(pretrained_model.s2t_train_args)\n",
    "del pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Since we need to create text with the new token `<intent>`, we need to modify the dump file generated by recipe. We want to modify the `text` to the following format: \n",
    "\n",
    "`<eng><intent><notimestamps> intent`\n",
    "\n",
    "To chieve this, we need to write a custom dataset class and `data_info` with functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, data_path, is_train=True):\n",
    "        self.data_path = data_path\n",
    "        if is_train:\n",
    "            data_path = f\"{data_path}/train\"\n",
    "        else:\n",
    "            data_path = f\"{data_path}/devel\"\n",
    "        \n",
    "        self.data = {}\n",
    "        with open(f\"{data_path}/wav.scp\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, audio_path = line.strip().split(maxsplit=1)\n",
    "                self.data[audio_id] = {\n",
    "                    'audio_path': audio_path\n",
    "                }\n",
    "\n",
    "        with open(f\"{data_path}/transcript\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, translated = line.strip().split(maxsplit=1)\n",
    "                self.data[audio_id]['transcript'] = translated\n",
    "        \n",
    "        with open(f\"{data_path}/text\", \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                audio_id, intent, _ = line.strip().split(maxsplit=2)\n",
    "                self.data[audio_id]['intent'] = intent\n",
    "        \n",
    "        self.keys = list(self.data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'audio_path': self.data[idx]['audio_path'],\n",
    "            'intent': self.data[idx]['intent'],\n",
    "            'transcript': self.data[idx]['transcript']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n",
    "\n",
    "data_info = {\n",
    "    \"speech\": lambda d : librosa.load(d['audio_path'], sr=16000)[0],\n",
    "    \"text\": lambda d : tokenize(f\"<eng><intent><notimestamps>{d['intent']}\"),\n",
    "    \"text_prev\": lambda d : tokenize(\"<na>\"),\n",
    "    \"text_ctc\": lambda d : tokenize(d['transcript'].lower()),\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "Let's prepare the `build_model_fn` function for the Trainer. We will use the OWSM-v3.1-ebf-base model, which has approxmately 100M parameters.\n",
    "\n",
    "Please note that we cannot initiate the model outside of a function. When we try to initialize the model with multi-GPU environment, we need to initialize model for each GPUs. For this reason, it is easier to run model initialization funtion for each GPU rather than copying the model. So we need to write initialization code inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fn(args):\n",
    "    pretrained_model = Speech2Text.from_pretrained(\n",
    "        FINETUNE_MODEL,\n",
    "        beam_size=10,\n",
    "    )\n",
    "    model = pretrained_model.s2t_model\n",
    "    model.train()\n",
    "    print(f'Trainable parameters: {count_parameters(model)}')\n",
    "    \n",
    "    # Add new <intent> token\n",
    "    _, _, new_embedding = ez.preprocess.add_special_tokens(\n",
    "        pretrained_model.tokenizer,\n",
    "        pretrained_model.converter,\n",
    "        model.decoder.embed[0],\n",
    "        ADDITIONAL_SPECIAL_TOKENS,\n",
    "        insert_after=\"<st_zho>\"\n",
    "    )\n",
    "    new_embedding.weight.requires_grad = True\n",
    "    model.decoder.embed[0] = new_embedding\n",
    "    # apply lora if you want.\n",
    "    # create_lora_adapter(model, target_modules=LORA_TARGET)\n",
    "    # print(f'Trainable parameters after LORA: {count_parameters(model)}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Now let's set up the training configuration for the OWSM finetuning. Basically all configurations are the same as the OWSM training, but we will change some parameters for this finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training config from the pretrained model.\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "    \"s2t\",\n",
    "    training_config,\n",
    "    args.config\n",
    ")\n",
    "finetune_config['multiple_iterator'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=True)\n",
    "dev_dataset = CustomDataset(data_path=\"./dump/raw\", is_train=False)\n",
    "\n",
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "dev_dataset = ez.dataset.ESPnetEZDataset(dev_dataset, data_info=data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we have everything prepared, we can start training the OWSM model for ST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task=\"s2t\",\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=dev_dataset,\n",
    "    data_info=data_info,\n",
    "    build_model_fn=build_model_fn,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1,\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
